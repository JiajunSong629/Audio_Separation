{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MsOS\n",
    "url = \"https://salford.figshare.com/ndownloader/articles/6901475/versions/4\"\n",
    "download_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    subprocess.run(f'wget {url}', shell=True)\n",
    "    subprocess.run(f'unzip 4', shell=True)\n",
    "    subprocess.run(f'unzip MSoS_challenge_2018_Development_v1-00.zip', shell=True)\n",
    "    subprocess.run(f'mv Development {download_path}', shell=True)\n",
    "    subprocess.run(f'rm MSoS*.zip', shell=True)\n",
    "    subprocess.run(f'rm *.csv', shell=True)\n",
    "    subprocess.run(f'rm 4', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source : Effects, \t Number of files: 300\n",
      "Source : Human, \t Number of files: 300\n",
      "Source : Music, \t Number of files: 300\n",
      "Source : Urban, \t Number of files: 300\n",
      "Source : Nature, \t Number of files: 300\n"
     ]
    }
   ],
   "source": [
    "# Audiofiles\n",
    "sources = ['Effects', 'Human', 'Music', 'Urban', 'Nature']\n",
    "AUDIOFILES = {}\n",
    "for source in sources:\n",
    "    source_dir = os.path.join(download_path, source)\n",
    "    AUDIOFILES[source] = [os.path.join(source_dir, filename) for filename in os.listdir(source_dir)]\n",
    "for source in sources:\n",
    "    print ('Source : {}, \\t Number of files: {}'.format(source, len(AUDIOFILES[source])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MUSDB 7s Sample Dataset to /home/jovyan/MUSDB18/MUSDB18-7...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.84704590e-02, -3.79333496e-02],\n",
       "       [-1.07421875e-01, -5.22460938e-02],\n",
       "       [-8.42590332e-02, -5.48400879e-02],\n",
       "       ...,\n",
       "       [-8.23974609e-04,  3.05175781e-05],\n",
       "       [-8.54492188e-04,  7.01904297e-04],\n",
       "       [-8.54492188e-04,  1.31225586e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import musdb\n",
    "mus = musdb.DB(download=True)\n",
    "mus[0].audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = random.choice(mus.tracks)\n",
    "track.chunk_duration = 5.0\n",
    "track.chunk_start = random.uniform(0, track.duration - track.chunk_duration)\n",
    "x = track.audio.T\n",
    "y = track.targets['vocals'].audio.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300032, 2)\n",
      "(220500, 2)\n"
     ]
    }
   ],
   "source": [
    "track = random.choice(mus.tracks)\n",
    "print (track.audio.shape)\n",
    "track.chunk_duration = 5.0\n",
    "track_chunk_start = random.uniform(0, track.duration - track.chunk_duration)\n",
    "print (track.audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocals          ==> SDR: -17.506  SIR:   7.700  ISR: -12.018  SAR:  -4.098  \n",
      "accompaniment   ==> SDR: -14.066  SIR: -16.224  ISR:  -0.706  SAR:  -4.105  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import museval\n",
    "# provide an estimate\n",
    "estimates = {\n",
    "    'vocals': np.random.random(track.audio.shape),\n",
    "    'accompaniment': np.random.random(track.audio.shape)\n",
    "}\n",
    "\n",
    "# evaluates using BSSEval v4, and writes results to `./eval`\n",
    "print(museval.eval_mus_track(track, estimates, output_dir=\"./eval\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSDB18 to DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix sources on the fly\n",
    "\n",
    "# important parameters :\n",
    "#     root              : ~/MUSDB18/MUSDB18-7\n",
    "#     split             : train or test\n",
    "#     samples_per_track : how many samples to take on one track (song)\n",
    "#     seq_duration      : how long the sample is in seconds\n",
    "#     target            : which source to isolate from the mix\n",
    "#     dtype             : torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUSDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, target, samples_per_track, seq_duration, dtype):\n",
    "        self.mus = musdb.DB(root = root, split = split)\n",
    "        self.split = split\n",
    "        self.samples_per_track = samples_per_track\n",
    "        self.seq_duration = seq_duration\n",
    "        self.dtype = dtype\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        audio_sources = []\n",
    "        track = self.mus.tracks[index // self.samples_per_track]\n",
    "        # at training time we assemble a custom mix\n",
    "        if self.split == 'train' and self.seq_duration:\n",
    "            for k, source in enumerate(self.mus.setup['sources']):\n",
    "                # memorize index of target source\n",
    "                if source == self.target:\n",
    "                    target_ind = k\n",
    "                \n",
    "                # set the excerpt duration\n",
    "                track.chunk_duration = self.seq_duration\n",
    "                # set random start position\n",
    "                track.chunk_start = random.uniform(0, track.duration - self.seq_duration)\n",
    "                # load source audio and apply time domain source_augmentations\n",
    "                audio = torch.tensor(track.sources[source].audio.T, dtype = self.dtype)\n",
    "                audio_sources.append(audio)\n",
    "            # create stem tensor of shape (source, channel, samples)\n",
    "            stems = torch.stack(audio_sources, dim=0)\n",
    "            # # apply linear mix over source index=0\n",
    "            x = stems.sum(0)\n",
    "            \n",
    "            if target_ind is not None:\n",
    "                y = stems[target_ind]\n",
    "            # assuming vocal/accompaniment scenario if target!=source\n",
    "            else:\n",
    "                vocind = list(self.mus.setup['sources'].keys()).index('vocals')\n",
    "                # apply time domain subtraction\n",
    "                y = x - stems[vocind]\n",
    "        \n",
    "        # for validation and test, we deterministically yield the full\n",
    "        # pre-mixed musdb track\n",
    "        else:\n",
    "            # get the non-linear source mix straight from musdb\n",
    "            x = torch.tensor(\n",
    "                track.audio.T,\n",
    "                dtype = self.dtype\n",
    "            )\n",
    "            y = torch.tensor(\n",
    "                track.targets[self.target].audio.T,\n",
    "                dtype = self.dtype\n",
    "            )\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mus.tracks) * self.samples_per_track\n",
    "\n",
    "    \n",
    "def load_dataset(root, samples_per_track, target, seq_duration, dtype):\n",
    "    train_dataset = MUSDBDataset(\n",
    "        root = root,\n",
    "        split='train',\n",
    "        samples_per_track = samples_per_track,\n",
    "        seq_duration = seq_duration,\n",
    "        target = target,\n",
    "        dtype = dtype\n",
    "    )\n",
    "\n",
    "    valid_dataset = MUSDBDataset(\n",
    "        root = root,\n",
    "        split='valid',\n",
    "        samples_per_track=1,\n",
    "        seq_duration = None,\n",
    "        target = target,\n",
    "        dtype = dtype\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(root = \"~/MUSDB18/MUSDB18-7\",\n",
    "                                           samples_per_track = 2,\n",
    "                                           seq_duration = 5,\n",
    "                                           dtype = torch.float32,\n",
    "                                           target = \"vocals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [05:50<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "total_training_duration = 0\n",
    "for k in tqdm.tqdm(range(len(train_dataset))):\n",
    "    x, y = train_dataset[k]\n",
    "    total_training_duration += x.shape[1] / 44100 #train_dataset.sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_training_duration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".audiosep",
   "language": "python",
   "name": ".audiosep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
